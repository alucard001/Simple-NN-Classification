{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install xlrd\n",
    "# !pip install spacy\n",
    "\n",
    "# https://github.com/Alex-CHUN-YU/Word2vec\n",
    "# !pip install gensim\n",
    "# !pip install jieba\n",
    "# !pip install thulac\n",
    "\n",
    "# !pip install textblob\n",
    "# !pip install snownlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import thulac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"data/dataset.xlsx\"\n",
    "xls_data = pd.read_excel(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>【曼市恐襲】耶耶托尼捐50萬助受害者　經理人：曼市無分曼城曼聯 面對恐襲，這一刻足球不再重要...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>《人民日報》：「特事特辦」還是少些好 特首梁振英幼女梁頌昕遺失行李，由航空公司職員到禁區外代...</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>特朗普、佩林雙劍合璧　變身政壇「雙頭怪」？ 特朗普和佩林看似風馬牛不相及，但其實兩人在政治立...</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【球場上的世界公民】講座精華節錄 2017年3月25日，港學堂與法國文化協會共同舉辦《世界公...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>【再見忠臣】迪拿達利告別烏甸尼斯　懷緬最動人12年 烏甸尼斯Vs卡比　5月16日（周一）　2...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content label\n",
       "0  【曼市恐襲】耶耶托尼捐50萬助受害者　經理人：曼市無分曼城曼聯 面對恐襲，這一刻足球不再重要...     C\n",
       "1  《人民日報》：「特事特辦」還是少些好 特首梁振英幼女梁頌昕遺失行李，由航空公司職員到禁區外代...     A\n",
       "2  特朗普、佩林雙劍合璧　變身政壇「雙頭怪」？ 特朗普和佩林看似風馬牛不相及，但其實兩人在政治立...     B\n",
       "3  【球場上的世界公民】講座精華節錄 2017年3月25日，港學堂與法國文化協會共同舉辦《世界公...     C\n",
       "4  【再見忠臣】迪拿達利告別烏甸尼斯　懷緬最動人12年 烏甸尼斯Vs卡比　5月16日（周一）　2...     C"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xls_data['content'][0]\n",
    "categories = set(xls_data['label'])\n",
    "\n",
    "with open('categories.pickle', 'wb') as handle:\n",
    "    pickle.dump(categories, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3894"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xls_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "CPU times: user 1min 31s, sys: 490 ms, total: 1min 31s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from bs4 import BeautifulSoup\n",
    "import thulac\n",
    "\n",
    "thu = thulac.thulac(seg_only=True)\n",
    "\n",
    "def clean_and_cut(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    new_text = soup.get_text()\n",
    "    \n",
    "    new_text_cut = thu.cut(new_text, text=True).replace(\"\\n\", \"\")\n",
    "    return new_text_cut\n",
    "\n",
    "xls_data['content_splited'] = xls_data['content'].apply(clean_and_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# thu = thulac.thulac(seg_only=True)\n",
    "# def ch_text_cut(t):\n",
    "#     return thu.cut(t)\n",
    "\n",
    "# xls_data['content_splited'] = xls_data['content'].apply(ch_text_cut)\n",
    "# xls_data['content_splited'][4].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(xls_data['content_splited'], xls_data['label'], \n",
    "#                                                     test_size=0.25, random_state=42)\n",
    "# full_train = [(k, v) for k, v in zip(X_train, y_train)]\n",
    "# full_test = [(k, v) for k, v in zip(X_test, y_test)]\n",
    "\n",
    "# for k, v in zip(X_train[0:10], y_train[0:10]):\n",
    "#     print(\"Key:\" + k)\n",
    "#     print(\"\\n\")\n",
    "#     print(\"Value:\" + v)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# full_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from textblob.classifiers import NaiveBayesClassifier\n",
    "# import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# cl = NaiveBayesClassifier(full_train)\n",
    "# cl.show_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(\"Accuracy: \", cl.accuracy(full_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "num_words = 3\n",
    "tokenizer = Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.fit_on_texts(xls_data['content_splited'])\n",
    "# tokenizer.texts_to_matrix(xls_data['content_splited'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.9 ms, sys: 1.94 ms, total: 6.84 ms\n",
      "Wall time: 5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_classes = len(set(xls_data['label']))\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(xls_data['label'])\n",
    "# print(integer_encoded)\n",
    "\n",
    "# Category encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "y = onehot_encoder.fit_transform(integer_encoded)\n",
    "# print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.29 s, sys: 48.5 ms, total: 3.34 s\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.fit_on_texts(xls_data['content_splited'])\n",
    "# X = tokenizer.texts_to_sequences(xls_data['content_splited'])\n",
    "\n",
    "# saving\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "X = tokenizer.texts_to_matrix(xls_data['content_splited'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 263 ms, sys: 104 ms, total: 367 ms\n",
      "Wall time: 364 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = np.max([len(x.split(\" \")) for x in xls_data['content_splited']])\n",
    "X = pad_sequences(X, maxlen=max_words, padding='post')\n",
    "\n",
    "# saving\n",
    "with open('max_words.pickle', 'wb') as handle:\n",
    "    pickle.dump(max_words, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def top_1_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=1)\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 32, input_length= X.shape[1]))\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', top_1_accuracy])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7647, 32)          4671808   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7647, 64)          2112      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7647, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7647, 128)         8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7647, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 978816)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 2936451   \n",
      "=================================================================\n",
      "Total params: 7,618,691\n",
      "Trainable params: 7,618,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3115 samples, validate on 779 samples\n",
      "Epoch 1/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.6010 - acc: 0.4622 - top_1_accuracy: 0.4622Epoch 00000: val_loss improved from inf to 1.03450, saving model to text_model.hdf5\n",
      "3115/3115 [==============================] - 85s - loss: 1.5927 - acc: 0.4639 - top_1_accuracy: 0.4639 - val_loss: 1.0345 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 2/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.0021 - acc: 0.5495 - top_1_accuracy: 0.5495Epoch 00001: val_loss improved from 1.03450 to 1.02589, saving model to text_model.hdf5\n",
      "3115/3115 [==============================] - 85s - loss: 1.0011 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0259 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 3/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.0024 - acc: 0.5511 - top_1_accuracy: 0.5511Epoch 00002: val_loss improved from 1.02589 to 1.02502, saving model to text_model.hdf5\n",
      "3115/3115 [==============================] - 83s - loss: 1.0029 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0250 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 4/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.0030 - acc: 0.5498 - top_1_accuracy: 0.5498Epoch 00003: val_loss did not improve\n",
      "3115/3115 [==============================] - 83s - loss: 1.0026 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0286 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 5/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 0.9999 - acc: 0.5514 - top_1_accuracy: 0.5514Epoch 00004: val_loss did not improve\n",
      "3115/3115 [==============================] - 84s - loss: 1.0005 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0318 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 6/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 0.9988 - acc: 0.5524 - top_1_accuracy: 0.5524Epoch 00005: val_loss improved from 1.02502 to 1.02349, saving model to text_model.hdf5\n",
      "3115/3115 [==============================] - 84s - loss: 1.0009 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0235 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 7/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.0032 - acc: 0.5505 - top_1_accuracy: 0.5505Epoch 00006: val_loss did not improve\n",
      "3115/3115 [==============================] - 83s - loss: 1.0031 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0237 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 8/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.0015 - acc: 0.5508 - top_1_accuracy: 0.5508Epoch 00007: val_loss did not improve\n",
      "3115/3115 [==============================] - 83s - loss: 1.0017 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0251 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 9/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.0014 - acc: 0.5501 - top_1_accuracy: 0.5501Epoch 00008: val_loss improved from 1.02349 to 1.02297, saving model to text_model.hdf5\n",
      "3115/3115 [==============================] - 83s - loss: 1.0010 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0230 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "Epoch 10/10\n",
      "3072/3115 [============================>.] - ETA: 1s - loss: 1.0010 - acc: 0.5505 - top_1_accuracy: 0.5505Epoch 00009: val_loss did not improve\n",
      "3115/3115 [==============================] - 83s - loss: 1.0011 - acc: 0.5506 - top_1_accuracy: 0.5506 - val_loss: 1.0290 - val_acc: 0.5237 - val_top_1_accuracy: 0.5237\n",
      "CPU times: user 1h 14min 45s, sys: 17min 26s, total: 1h 32min 12s\n",
      "Wall time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "# results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "model = baseline_model()\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "checkPoint = ModelCheckpoint(filepath='text_model.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=10, validation_split=0.2, shuffle=True, callbacks=[checkPoint])\n",
    "model.load_weights('text_model.hdf5')\n",
    "\n",
    "model.save(\"text_full_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3894/3894 [==============================] - 19s    \n",
      "CPU times: user 1min 22s, sys: 5.39 s, total: 1min 27s\n",
      "Wall time: 19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.23976718,  0.22282586,  0.53740704],\n",
       "       [ 0.23976718,  0.22282586,  0.53740704],\n",
       "       [ 0.23976718,  0.22282586,  0.53740704],\n",
       "       ..., \n",
       "       [ 0.23976718,  0.22282586,  0.53740704],\n",
       "       [ 0.23976718,  0.22282583,  0.53740704],\n",
       "       [ 0.23976718,  0.22282583,  0.53740704]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再 發 霜 凍 警告 21 人 低 溫 入院 兩 人 危殆寒冷 天氣 持續 ， 天文台 在 下午 再度 發出霜 凍 警告 ， 指明 早 新界 北部 可能 出現 地面霜 。 寒冷 天氣 警告 及 紅色 火災 危險 警告 繼續 生效 。 天文台 預測 ， 明早市 區 最低 氣溫約 8 度 ， 新界 再 低 兩 三 度 ， 日間 最高 氣溫約 13 度 。 有 菜 農指 ， 若 果結霜 ， 只能 提 早 收割 。醫管局 指 ， 在 截至 下午 五時 的 過 去 二十四 小 時 ， 公立 醫院 合共 收到 二十一 名 病人 因 低溫 而 需 送 往 急症室 治療 ， 包括 九 男 十二 女 ， 年齡 介乎 五十三 至 九十三 歲 ， 當 中 兩 人 危殆 、 六 人 嚴重 。\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    load_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('max_words.pickle', 'rb') as handle:\n",
    "    load_max_words = pickle.load(handle)\n",
    "    \n",
    "example_text = \"\"\"\n",
    "再發霜凍警告21人低溫入院兩人危殆\n",
    "\n",
    "寒冷天氣持續，天文台在下午再度發出霜凍警告，指明早新界北部可能出現地面霜。寒冷天氣警告及紅色火災危險警告繼續生效。天文台預測，明早市區最低氣溫約8度，新界再低兩三度，日間最高氣溫約13度。有菜農指，若果結霜，只能提早收割。\n",
    "\n",
    "醫管局指，在截至下午五時的過去二十四小時，公立醫院合共收到二十一名病人因低溫而需送往急症室治療，包括九男十二女，年齡介乎五十三至九十三歲，當中兩人危殆、六人嚴重。\n",
    "\"\"\"\n",
    "new_text = clean_and_cut(example_text)\n",
    "print(new_text)\n",
    "\n",
    "new_X = load_tokenizer.texts_to_matrix(new_text)\n",
    "new_X = pad_sequences(new_X, maxlen=load_max_words, padding='post')\n",
    "print(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338/338 [==============================] - 1s     \n",
      "338/338 [==============================] - 1s     \n",
      "CPU times: user 21 s, sys: 1.38 s, total: 22.4 s\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict = model.predict(new_X)\n",
    "predict_prob = model.predict_proba(new_X)\n",
    "predict_class = model.predict_classes(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predict[np.argmax(predict)])\n",
    "# print(predict_prob[np.argmax(predict_prob)])\n",
    "# print(predict_class[np.argmax(predict_class)])\n",
    "from collections import Counter\n",
    "cls_counter = Counter(predict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = cls_counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(list(categories))[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
